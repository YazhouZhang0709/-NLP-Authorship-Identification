{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Related Libraries for Text Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "import operator\n",
    "import textwrap\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.vq import whiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "### 10 English books with labels from 0~9 (can be found in the folders)\n",
    "###  Book Labels, Book Names and Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | Book | Author |\n",
    "| ---- | ---- | ---- |\n",
    "| 0 | A Tale of Two Cities | Charles Dickens |\n",
    "| 1 | Meditations | Marcus Aurelius |\n",
    "| 2 | Dracula | Bram Stoker |\n",
    "| 3 | Grimms' Fairy Tales | Grimm brothers |\n",
    "| 4 | The Practice and Science of Drawing | Harold Speed |\n",
    "| 5 | Pride and Prejudice | Jane Austen |\n",
    "| 6 | Beyond Good and Evil | Friedrich Nietzsche |\n",
    "| 7 | Dubliners | James Joyce |\n",
    "| 8 | The Souls of Black Folk | W. E. B. Du Bois |\n",
    "| 9 | The Picture of Dorian Gray | Oscar Wilde |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Create a dictionary for the final prediction\n",
    "Book_dict={0:\"Charles Dickens\", 1:\"Marcus Aurelius\", 2:\"Bram Stoker\",\n",
    "           3:\"Grimm brothers\", 4:\"Harold Speed\", 5:\"Jane Austen\",\n",
    "           6:\"Friedrich Nietzsche\", 7:\"James Joyce\", 8:\"W. E. B. Du Bois\", 9:\"Oscar Wilde\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From these 10 books, here I pick every 2000 strings as one sample, attached with the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\\$0$ A_Tale_of_Two_Cities.txt\n",
      "Data\\$1$ Meditations.txt\n",
      "Data\\$2$ Dracula.txt\n",
      "Data\\$3$ Grimms'_Fairy_Tales.txt\n",
      "Data\\$4$ The_Practice_and_Science_of_Drawing.txt\n",
      "Data\\$5$ Pride_and_prejudice.txt\n",
      "Data\\$6$ Beyond_Good_And_Evil.txt\n",
      "Data\\$7$ Dubliners.txt\n",
      "Data\\$8$ The_Souls_of_Black_Folk.txt\n",
      "Data\\$9$ The_Picture_of_Dorian_Gray.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a list of sample labels representing the authorships\n",
    "sample_labels=[] \n",
    "# Create a list of samples\n",
    "samples = []\n",
    "# Load data from the folder\n",
    "files = glob.glob(os.path.join(\"Data\", \"*.txt\"))\n",
    "for fn in files:\n",
    "    with open(fn, encoding=\"utf8\") as f:\n",
    "            print(fn)\n",
    "            for segment in textwrap.wrap(f.read().replace('\\n',' '),2000):\n",
    "                samples.append(segment)\n",
    "                sample_labels.append(int(fn.split('$')[1]))\n",
    "all_text = ''.join(str(samples))\n",
    "num_samples = len(samples) # 2370 samples in total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "## Based on my literature survey and online resource seaching, here we define 3 functions to extract different features from the text segments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lexical and Punctuation features\n",
    "- ### Lexical features:\n",
    "    - #### The average number of words per sentence\n",
    "    - #### Sentence length variation\n",
    "    - #### Lexical diversity, which is a measure of the richness of the authorâ€™s vocabulary\n",
    "- ### Punctuation features:\n",
    "    - #### Average number of commas, semicolons and colons etc. per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def Lexical_Punctuation(data):\n",
    "    # Creatte feature vector           \n",
    "    fvs_lexical = np.zeros((len(data),3), np.float64)\n",
    "    fvs_punct = np.zeros((len(data),5), np.float64)\n",
    "    for e, ch_text in enumerate(data):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        tokens = nltk.word_tokenize(ch_text.lower())\n",
    "        words = word_tokenizer.tokenize(ch_text.lower()) # words without punctuation\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "\n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "        # Colons per sentence\n",
    "        fvs_punct[e, 2] = tokens.count(':') / float(len(sentences))\n",
    "        # Question marks per sentence\n",
    "        fvs_punct[e, 3] = tokens.count('?') / float(len(sentences))\n",
    "        # Question marks per sentence\n",
    "        fvs_punct[e, 4] = tokens.count('\"') / float(len(sentences))\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "    \n",
    "    return fvs_lexical, fvs_punct\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words features\n",
    "###   Bag of words represents the frequencies of different words in each chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common words in all books\n",
    "NUM_TOP_WORDS = 10\n",
    "all_tokens = nltk.word_tokenize(all_text)\n",
    "fdist = nltk.FreqDist(all_tokens)\n",
    "vocab = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True) \n",
    "vocab = list(dict(vocab).keys())[:NUM_TOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to create the bag for words feature vector for each chapter\n",
    "vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "def Bag_of_words(data):\n",
    "    fvs_bow = vectorizer.fit_transform(data).toarray().astype(np.float64)\n",
    "    # normalise by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "    return fvs_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Synthetic Features\n",
    "###   For the last feature, here I extract syntactic features of the text. Part of speech (POS) is a classification of each token into a lexical category (e.g. noun). NLTK has a function for POS labeling, and our feature vector is comprised of frequencies for the most common POS tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get part of speech for each token in each chapter\n",
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch)\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "def Synthetic_features(data):\n",
    "    data_pos = [token_to_pos(ch) for ch in data]\n",
    "\n",
    "    # count frequencies for common POS types\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                           for ch in data_pos]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens in the books\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in data_pos])]\n",
    "    \n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: MLP/SVM/KNN/RandomForest based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "##   Build a classifier and then vote for \n",
    "def Author_Predictor_SVM(fvs_train, fvs_test, y_train):\n",
    "    svcm = SVC()\n",
    "    svcm.fit(fvs_train,y_train)\n",
    "    return svcm.predict(fvs_test)\n",
    "\n",
    "def Author_Predictor_MLP(fvs_train, fvs_test, y_train):\n",
    "    MLP = MLPClassifier(hidden_layer_sizes=(150, ),max_iter=200)\n",
    "    MLP.fit(fvs_train,y_train)\n",
    "    return MLP.predict(fvs_test)\n",
    "\n",
    "def Author_Predictor_RF(fvs_train, fvs_test, y_train):\n",
    "    rf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=15)\n",
    "    rf.fit(fvs_train,y_train)\n",
    "    return rf.predict(fvs_test)\n",
    "\n",
    "def Author_Predictor_KNN(fvs_train, fvs_test, y_train):\n",
    "    KNN = KNeighborsClassifier(20)\n",
    "    KNN.fit(fvs_train,y_train)\n",
    "    return KNN.predict(fvs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing data split Using 5-fold Cross Validation here\n",
    "### 5 different splits of data set, in each split :80% for training, remaining 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\scipy\\cluster\\vq.py:141: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************Result for Fold2***************************************\n",
      "The test accuracy for Fold2is 0.572992700729927\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Charles Dickens       0.39      0.73      0.51        78\n",
      "    Marcus Aurelius       1.00      0.19      0.32        42\n",
      "        Bram Stoker       0.70      0.22      0.33        87\n",
      "     Grimm brothers       0.92      0.84      0.88        55\n",
      "       Harold Speed       0.74      0.61      0.67        46\n",
      "        Jane Austen       0.67      0.54      0.59        71\n",
      "Friedrich Nietzsche       0.40      0.78      0.52        41\n",
      "        James Joyce       0.86      0.49      0.62        39\n",
      "   W. E. B. Du Bois       0.43      0.72      0.54        43\n",
      "        Oscar Wilde       0.81      0.83      0.82        46\n",
      "\n",
      "        avg / total       0.68      0.58      0.57       548\n",
      "\n",
      "[[64  0  0  1  1  1  7  0  3  1]\n",
      " [ 5  8  1  0  0  6 14  0  7  1]\n",
      " [43  0 15  0  2  6  1  0 17  3]\n",
      " [ 7  0  0 48  0  0  0  0  0  0]\n",
      " [ 3  1  0  0 25  3  3  2  7  2]\n",
      " [28  0  1  0  0 36  5  0  1  0]\n",
      " [ 5  0  0  0  0  2 29  1  4  0]\n",
      " [ 0  0  7  1  4  0  0 16  7  4]\n",
      " [ 6  0  0  0  1  0  3  0 33  0]\n",
      " [ 7  0  0  0  0  0  1  0  0 38]]\n",
      "***************************Result for Fold2***************************************\n",
      "***************************Result for Fold3***************************************\n",
      "The test accuracy for Fold3is 0.7271062271062271\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Charles Dickens       0.60      0.68      0.63        78\n",
      "    Marcus Aurelius       0.86      0.43      0.57        42\n",
      "        Bram Stoker       0.93      0.45      0.60        87\n",
      "     Grimm brothers       0.84      0.94      0.89        54\n",
      "       Harold Speed       0.89      0.85      0.87        46\n",
      "        Jane Austen       0.58      0.99      0.73        71\n",
      "Friedrich Nietzsche       0.64      0.71      0.67        41\n",
      "        James Joyce       0.76      0.87      0.81        39\n",
      "   W. E. B. Du Bois       0.70      0.55      0.61        42\n",
      "        Oscar Wilde       0.87      0.85      0.86        46\n",
      "\n",
      "        avg / total       0.76      0.72      0.71       546\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  0  0  3  1 15  1  0  2  1]\n",
      " [ 8 26  1  2  0  1  3  0  1  0]\n",
      " [17  0 41  5  1 16  0  3  2  2]\n",
      " [ 4  0  0 50  0  0  0  0  0  0]\n",
      " [ 0  0  0  2 40  1  1  2  0  0]\n",
      " [ 2  0  0  0  0 68  1  0  0  0]\n",
      " [ 1  3  0  0  0  4 28  2  1  2]\n",
      " [ 2  0  2  1  1  0  0 33  0  0]\n",
      " [10  0  0  0  0  4  8  0 20  0]\n",
      " [ 2  0  0  0  0  1  0  1  1 41]]\n",
      "***************************Result for Fold3***************************************\n",
      "***************************Result for Fold4***************************************\n",
      "The test accuracy for Fold4is 0.7371323529411765\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Charles Dickens       0.55      0.85      0.67        78\n",
      "    Marcus Aurelius       0.92      0.83      0.88        42\n",
      "        Bram Stoker       0.76      0.75      0.75        87\n",
      "     Grimm brothers       0.90      0.98      0.94        54\n",
      "       Harold Speed       0.64      0.91      0.75        46\n",
      "        Jane Austen       0.89      0.56      0.69        71\n",
      "Friedrich Nietzsche       0.74      0.57      0.65        40\n",
      "        James Joyce       0.88      0.56      0.69        39\n",
      "   W. E. B. Du Bois       0.76      0.81      0.78        42\n",
      "        Oscar Wilde       0.93      0.62      0.75        45\n",
      "\n",
      "        avg / total       0.78      0.75      0.75       544\n",
      "\n",
      "[[60  0  2  4  0  0  3  2  7  0]\n",
      " [ 0 38  2  0  0  0  2  0  0  0]\n",
      " [ 9  4 62  2  4  2  0  3  1  0]\n",
      " [ 0  0  1 53  0  0  0  0  0  0]\n",
      " [ 1  0  1  0 42  1  0  1  0  0]\n",
      " [13  2  4  1  1 44  2  1  3  0]\n",
      " [ 5  0  0  4  3  1 25  0  2  0]\n",
      " [ 4  0  4  0  4  1  0 25  0  1]\n",
      " [ 0  0  1  0  2  0  1  0 37  1]\n",
      " [ 3  0  0  0  6  0  3  4  2 27]]\n",
      "***************************Result for Fold4***************************************\n",
      "***************************Result for Fold5***************************************\n",
      "The test accuracy for Fold5is 0.7527675276752768\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Charles Dickens       0.55      0.68      0.61        78\n",
      "    Marcus Aurelius       0.91      0.69      0.78        42\n",
      "        Bram Stoker       0.84      0.61      0.71        87\n",
      "     Grimm brothers       0.93      0.76      0.84        54\n",
      "       Harold Speed       0.76      0.93      0.84        45\n",
      "        Jane Austen       0.75      0.89      0.81        70\n",
      "Friedrich Nietzsche       0.54      0.97      0.70        40\n",
      "        James Joyce       0.88      0.56      0.69        39\n",
      "   W. E. B. Du Bois       0.69      0.57      0.62        42\n",
      "        Oscar Wilde       0.86      0.69      0.77        45\n",
      "\n",
      "        avg / total       0.76      0.73      0.73       542\n",
      "\n",
      "[[58  1  3  0  0  3  8  0  5  0]\n",
      " [ 4 31  0  0  1  0  4  0  1  1]\n",
      " [11  2 49  1  3  9  2  2  7  1]\n",
      " [10  0  0 41  0  1  2  0  0  0]\n",
      " [ 1  1  1  0 40  0  1  0  1  0]\n",
      " [10  0  1  0  0 57  2  0  0  0]\n",
      " [ 0  0  0  0  0  0 39  0  1  0]\n",
      " [ 5  0  3  0  3  0  2 24  0  2]\n",
      " [ 4  0  0  0  6  0  7  0 25  0]\n",
      " [ 4  0  1  0  0  3  2  3  2 30]]\n",
      "***************************Result for Fold5***************************************\n",
      "***************************Result for Fold6***************************************\n",
      "The test accuracy for Fold6is 0.49074074074074076\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Charles Dickens       0.53      0.21      0.30        77\n",
      "    Marcus Aurelius       0.16      0.17      0.16        42\n",
      "        Bram Stoker       0.42      0.78      0.55        86\n",
      "     Grimm brothers       0.86      0.78      0.82        54\n",
      "       Harold Speed       0.35      0.64      0.45        45\n",
      "        Jane Austen       0.86      0.44      0.58        70\n",
      "Friedrich Nietzsche       0.33      0.12      0.18        40\n",
      "        James Joyce       0.40      0.69      0.50        39\n",
      "   W. E. B. Du Bois       0.67      0.05      0.09        42\n",
      "        Oscar Wilde       0.67      0.78      0.72        45\n",
      "\n",
      "        avg / total       0.54      0.48      0.45       540\n",
      "\n",
      "[[18  6 34  0  8  0  1  3  0  7]\n",
      " [ 1  6 17  0  7  0  2  3  0  6]\n",
      " [ 0  8 65  0  8  1  1  2  0  1]\n",
      " [ 1  1  3 39  7  0  1  1  1  0]\n",
      " [ 0  3  3  0 30  0  5  4  0  0]\n",
      " [ 5  5 24  0  6 24  1  2  0  3]\n",
      " [ 1 22  1  0  8  0  6  1  1  0]\n",
      " [ 1  3  1  0  8  0  1 23  0  2]\n",
      " [ 2  6 14  2  7  0  2  3  6  0]\n",
      " [ 0  0  0  0  7  0  0  3  0 35]]\n",
      "***************************Result for Fold6***************************************\n"
     ]
    }
   ],
   "source": [
    "samples = np.array(samples)\n",
    "sample_labels = np.array(sample_labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "accuracy_result = []\n",
    "classification_report_result = []\n",
    "confusion_matrix_result = []\n",
    "target_names = [Book_dict[i] for i in range(10)]\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "fold_n = 0\n",
    "for train_index, test_index in skf.split(samples, sample_labels):\n",
    "    X_train, X_test = samples[train_index],samples[test_index]\n",
    "    y_train, y_test = sample_labels[train_index], sample_labels[test_index]\n",
    "    \n",
    "    # Here we concatenate all these four different feature vectors together to get the final feature vectors\n",
    "    # Feature vectors for training\n",
    "    train_fvs = np.hstack((Lexical_Punctuation(X_train)[0],Lexical_Punctuation(X_train)[1], Bag_of_words(X_train), Synthetic_features(X_train)))\n",
    "    # Feature vectors for testing\n",
    "    test_fvs = np.hstack((Lexical_Punctuation(X_test)[0],Lexical_Punctuation(X_test)[1], Bag_of_words(X_test), Synthetic_features(X_test)))\n",
    "    \n",
    "    # Show the 5 fold validation results of our model\n",
    "    fold_n += 1\n",
    "    print(\"***************************\"+\"Result for Fold \"+str(fold_n)+\"***************************************\")\n",
    "    acc = accuracy_score(y_test, Author_Predictor_MLP(train_fvs, test_fvs, y_train))\n",
    "    print(\"The test accuracy for Fold\"+str(fold_n)+\"is\", acc)\n",
    "    accuracy_result.append(acc)\n",
    "    cla_report = classification_report(y_test, Author_Predictor_MLP(train_fvs, test_fvs, y_train), target_names = target_names )\n",
    "    print(cla_report)\n",
    "    classification_report_result.append(cla_report)\n",
    "    con_matrix = confusion_matrix(y_test, Author_Predictor_MLP(train_fvs, test_fvs, y_train))\n",
    "    print(con_matrix)\n",
    "    confusion_matrix_result.append(con_matrix)\n",
    "    print(\"***************************\"+\"Result for Fold \"+str(fold_n)+\"***************************************\")\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.572992700729927,\n",
       " 0.7271062271062271,\n",
       " 0.7371323529411765,\n",
       " 0.7527675276752768,\n",
       " 0.49074074074074076]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy of the test data is: 0.6561479098386697\n"
     ]
    }
   ],
   "source": [
    "avg_accuracy = np.mean(accuracy_result)\n",
    "print(\"The average accuracy of the test data is:\", avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, Author_Predictor_MLP(train_fvs, test_fvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [Book_dict[i] for i in range(10)]\n",
    "\n",
    "print(classification_report(y_test, Author_Predictor_MLP(train_fvs, test_fvs), target_names = target_names ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning: KMeans Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=10, init='k-means++', n_init=10, verbose=0)\n",
    "km.fit(fvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
